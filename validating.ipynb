{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Processed 100/1749 images\n",
      "Processed 200/1749 images\n",
      "Processed 300/1749 images\n",
      "Processed 400/1749 images\n",
      "Processed 500/1749 images\n",
      "Processed 600/1749 images\n",
      "Processed 700/1749 images\n",
      "Processed 800/1749 images\n",
      "Processed 900/1749 images\n",
      "Processed 1000/1749 images\n",
      "Processed 1100/1749 images\n",
      "Processed 1200/1749 images\n",
      "Processed 1300/1749 images\n",
      "Processed 1400/1749 images\n",
      "Processed 1500/1749 images\n",
      "Processed 1600/1749 images\n",
      "Processed 1700/1749 images\n",
      "Processed 1749/1749 images\n",
      "\n",
      "Evaluation Results:\n",
      "Precision: 1.0000\n",
      "Recall: 0.8620\n",
      "F1 Score: 0.9259\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "model = YOLO('models/yolov8n_threat_detection.pt')\n",
    "\n",
    "class_names = ['ammo', 'firearm', 'grenade', 'knife', 'pistol', 'rocket']\n",
    "\n",
    "def detect_with_nms(image_path, iou_threshold=0.60):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = model(image, verbose=False)[0]\n",
    "        \n",
    "    nms_detections = sv.Detections.from_ultralytics(results).with_nms(threshold=iou_threshold)\n",
    "    \n",
    "    return nms_detections\n",
    "\n",
    "def calculate_metrics(predictions, targets):\n",
    "    pred_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_counter.update(pred['labels'].tolist())\n",
    "        target_counter.update(target['labels'].tolist())\n",
    "\n",
    "    all_classes = set(pred_counter.keys()) | set(target_counter.keys())\n",
    "    \n",
    "    tp = sum(min(pred_counter[c], target_counter[c]) for c in all_classes)\n",
    "    fp = sum(max(pred_counter[c] - target_counter[c], 0) for c in all_classes)\n",
    "    fn = sum(max(target_counter[c] - pred_counter[c], 0) for c in all_classes)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def evaluate_dataset(dataset_path):\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    images_path = os.path.join(dataset_path, 'images')\n",
    "    labels_path = os.path.join(dataset_path, 'labels')\n",
    "\n",
    "    image_files = [f for f in os.listdir(images_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    total_images = len(image_files)\n",
    "\n",
    "    for i, img_file in enumerate(image_files):\n",
    "        img_path = os.path.join(images_path, img_file)\n",
    "        txt_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        txt_path = os.path.join(labels_path, txt_file)\n",
    "        \n",
    "        try:\n",
    "            detections = detect_with_nms(img_path)\n",
    "            \n",
    "            pred = {\n",
    "                'boxes': torch.from_numpy(detections.xyxy),\n",
    "                'scores': torch.from_numpy(detections.confidence),\n",
    "                'labels': torch.from_numpy(detections.class_id)\n",
    "            }\n",
    "            \n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                gt_boxes = []\n",
    "                gt_labels = []\n",
    "                for line in lines:\n",
    "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
    "                    gt_boxes.append([x, y, x+w, y+h])\n",
    "                    gt_labels.append(int(class_id))\n",
    "                \n",
    "                target = {\n",
    "                    'boxes': torch.tensor(gt_boxes),\n",
    "                    'labels': torch.tensor(gt_labels)\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Warning: No annotation file found for {img_file}\")\n",
    "                target = {\n",
    "                    'boxes': torch.zeros((0, 4)),\n",
    "                    'labels': torch.zeros(0, dtype=torch.long)\n",
    "                }\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            targets.append(target)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "        \n",
    "        if (i + 1) % 100 == 0 or (i + 1) == total_images:\n",
    "            print(f\"Processed {i + 1}/{total_images} images\")\n",
    "\n",
    "    if predictions and targets:\n",
    "        metrics = calculate_metrics(predictions, targets)\n",
    "    else:\n",
    "        print(\"Error: No valid predictions or ground truth found.\")\n",
    "        metrics = {\n",
    "            'mAP@0.5': 0.0,\n",
    "            'Precision': 0.0,\n",
    "            'Recall': 0.0,\n",
    "            'F1 Score': 0.0\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "dataset_path = 'datasets/dangerous-objects/valid'\n",
    "print(\"Starting evaluation...\")\n",
    "results = evaluate_dataset(dataset_path)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
